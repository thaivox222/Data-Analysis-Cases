{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт из PostgreSQL в Google Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import DictCursor\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая функция подключается к БД и собирает названия столбцов из таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_columns_names():\n",
    "    columns = []\n",
    "    with psycopg2.connect(dbname='super_store_db1', user='XXX', \n",
    "                        password='XXX', host='XXX.eu-central-1.rds.amazonaws.com') as conn:\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cursor:        \n",
    "            cursor.execute(\"SELECT *FROM information_schema.columns WHERE table_schema = 'rxl' AND table_name = 'points';\")\n",
    "            for row in cursor:\n",
    "                columns.append(row[3])\n",
    "    print(columns)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pid', 'incomingtrackid', 'speed', 'latitude', 'longitude', 'pointdate', 'height', 'acceleration', 'deceleration', 'ticktimestamp', 'accelerationxoriginal', 'accelerationyoriginal', 'accelerationzoriginal']\n"
     ]
    }
   ],
   "source": [
    "make_columns_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая функция создает промежуточные csv-файлы с содержимым таблицы-источника\n",
    "и затем импортирует данные из них в Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_import():\n",
    "    scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('XXXXXXXXX.json', scope)\n",
    "    gc = gspread.authorize(creds)\n",
    "    with psycopg2.connect(dbname='super_store_db1', user='XXX', \n",
    "                            password='XXX', host='XXXXX.eu-central-1.rds.amazonaws.com') as conn:\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cursor:\n",
    "            cursor.execute('select * from rxl.points limit 5000;')\n",
    "            for row in cursor:\n",
    "                df_points = pd.DataFrame(cursor, columns=columns)\n",
    "            df_points.to_csv('points_psg.csv', index=False)\n",
    "            content = open('points_psg.csv', 'r').read()\n",
    "            gc.import_csv('XXX-dmc', content.encode('utf-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем всю связку\n",
    "def run_all():\n",
    "    make_columns_names()\n",
    "    run_import()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Версия скрипта для трех таблиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''версия для трех таблиц'''\n",
    "def make_columns_names_x3():\n",
    "    columns_p = []\n",
    "    columns_t = []\n",
    "    columns_o = []\n",
    "    with psycopg2.connect(dbname='super_store_db1', user='XXX', \n",
    "                        password='XXX', host='XXXXX.eu-central-1.rds.amazonaws.com') as conn:\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cursor:        \n",
    "            cursor.execute(\"SELECT *FROM information_schema.columns WHERE table_schema = 'rxl' AND table_name = 'points';\")\n",
    "            for row in cursor:\n",
    "                columns_p.append(row[3])\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cursor:        \n",
    "            cursor.execute(\"SELECT *FROM information_schema.columns WHERE table_schema = 'rxl' AND table_name = 'tracks';\")\n",
    "            for row in cursor:\n",
    "                columns_t.append(row[3])\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cursor:        \n",
    "            cursor.execute(\"SELECT *FROM information_schema.columns WHERE table_schema = 'rxl' AND table_name = 'os';\")\n",
    "            for row in cursor:\n",
    "                columns_o.append(row[3])                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_import_x3():\n",
    "    scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('XXXX.json', scope)\n",
    "    gc = gspread.authorize(creds)\n",
    "    with psycopg2.connect(dbname='super_store_db1', user='XXX', \n",
    "                            password='XXX', host='XXXXX.eu-central-1.rds.amazonaws.com') as conn:\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cursor:\n",
    "            cursor.execute('select * from rxl.points limit 5000;')\n",
    "            for row in cursor:\n",
    "                df_points = pd.DataFrame(cursor, columns=columns_p)\n",
    "            df_points.to_csv('points_psg.csv', index=False)\n",
    "            content = open('points_psg.csv', 'r', encoding=\"utf-8\").read()\n",
    "            gc.import_csv('XXX-dmc', content.encode('utf-8'))\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cursor:\n",
    "            cursor.execute('select * from rxl.tracks;')\n",
    "            for row in cursor:\n",
    "                df_tracks = pd.DataFrame(cursor, columns=columns_t)\n",
    "            df_tracks.to_csv('tracks_psg.csv', index=False)\n",
    "            content = open('tracks_psg.csv', 'r', encoding=\"utf-8\").read()\n",
    "            gc.import_csv('XXX', content.encode('utf-8'))\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cursor:\n",
    "            cursor.execute('select * from rxl.os;')\n",
    "            for row in cursor:\n",
    "                df_os = pd.DataFrame(cursor, columns=columns_o)\n",
    "            df_os.to_csv('os_psg.csv', index=False)\n",
    "            content = open('os_psg.csv', 'r').read()\n",
    "            gc.import_csv('XXX', content.encode('utf-8'))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Every 1 day at 12:02:00 do run_all() (last run: [never], next run: 2020-11-27 12:02:00)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import schedule\n",
    "schedule.every().day.at(\"12:02\").do(run_all).tag('daily_import_sheets')\n",
    "\n",
    "# при запуске цикла job становится сразу же на мониторинг\n",
    "#while True:\n",
    "    #schedule.run_pending()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отмена расписания (два варианта)\n",
    "#schedule.clear('daily_import_sheets')\n",
    "schedule.clear(run_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
